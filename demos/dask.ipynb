{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://gateway.dask.org/_static/images/dask-horizontal-white.svg\"\n",
    "     alt=\"Dask Logo\"\n",
    "     style=\"margin-right: 10px; width: 50%\" />\n",
    "# Distributed computing with Dask\n",
    "\n",
    "EODC offers Dask as service by utilising [Dask Gateway](https://gateway.dask.org/). User can launch a Dask cluster in a shared and managed cluster environment without requring to have direct access to any cloud infrastructure resources such as VMs or Kubernetes clusters. The objetive is to lower the entrance barrier for users to run large scale data analysis on demand and in a scaleable environment.\n",
    "\n",
    "An generic introduction of the usage of Dask Gateway can be found on the official [Dask Gateway documentation](https://gateway.dask.org/usage.html). In the following we will demonstrate the use of the Dask service at EODC to further support users.\n",
    "\n",
    "Pre-requisit is to have Dask Gateway installed in your environment\n",
    "```bash\n",
    "pip install dask-gateway\n",
    "```\n",
    "or \n",
    "```bash\n",
    "conda install -c conda-forge dask-gateway\n",
    "```\n",
    "\n",
    "It is important to note that the Python environment running the code and the environment utilised by Dask Gateway have to be almost identical.\n",
    "\n",
    "We will install some additional packages used in this demo afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication via OIDC password grant flow\n",
    "Only authenticated access is granted to the EODC Dask service, therefore a helper class to authenticate a user against the EODC identifiy managment system is implemented in the [EODC SDK](https://github.com/eodcgmbh/eodc-sdk).\n",
    "The users password is directly handed over to the request object and is not stored.\n",
    "Refreshed token is used to request a new access token in case it is expired, which is handled automatically in the authenticator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to EODC Dask\n",
    "\n",
    "Authenticating and connecting to EODC Dask can be done with a few lines of Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install eodc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following in order to make sure all dependencies are met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install bokeh==2.4.2 dask-gateway==2023.1.1 cloudpickle==2.2.1 s3fs==2023.6.0 fsspec==2023.6.0 xarray==2023.7.0 dask==2023.8.0 distributed==2023.8.0 lz4==4.3.2 tornado==6.3.2 rich rioxarray zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from eodc.dask import EODCDaskGateway\n",
    "from rich.console import Console\n",
    "from rich.prompt import Prompt\n",
    "console = Console()\n",
    "your_username = Prompt.ask(prompt=\"Enter your Username\")\n",
    "gateway = EODCDaskGateway(username=your_username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Cluster configuration if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_options = gateway.cluster_options()\n",
    "cluster_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dask Cluster\n",
    "\n",
    "Now we are going to create a Dask Cluster in order to run compute jobs.\n",
    "To communicate with the cluster we have to instantiate a client as well.\n",
    "Per default, no worker nodes are spawned, but this can be done either manually or even by enabling adaptive scaling of the cluster.\n",
    "\n",
    "**Important: Please use the widget to add/scale the Dask workers. Per default no worker is spawned, therefore no computations can be performed by the cluster.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = gateway.new_cluster(cluster_options)\n",
    "client = cluster.get_client()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to spawn a workers directly via Python adaptively please use the following method call. With the following the cluster will be scaled to 2 workers initially.\n",
    "Depending on the load, Dask will add addtional workers, up to 5, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.adapt(minimum=2, maximum=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List clusters if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "console.print(gateway.list_clusters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can connect to already running clusters again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = gateway.connect(gateway.list_clusters()[0].name)\n",
    "console.print(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Dask Dashboard to monitor execution of computations\n",
    "Copy the following link into a browser of your choice. Please consider the dashboard url provided is making use of http and not https."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.dashboard_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import xarray as xr\n",
    "\n",
    "s3fs_central = s3fs.S3FileSystem(\n",
    "    anon=True,\n",
    "    use_ssl=True,\n",
    "    client_kwargs={\"endpoint_url\": \"https://s3.central.data.destination-earth.eu\"})\n",
    "\n",
    "s3fs_lumi = s3fs.S3FileSystem(\n",
    "    anon=True,\n",
    "    use_ssl=True,\n",
    "    client_kwargs={\"endpoint_url\": \"https://s3.lumi.data.destination-earth.eu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3fs_central.ls(\"increment1-testdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data stored in S3 bucket at central site (Poland).\n",
    "The data we want to read is a single Zarr data store representing GFM flood data over Pakistan for 2022-08-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flood_map = xr.open_zarr(store=s3fs.S3Map(root=f\"increment1-testdata/2022-08-30.zarr\", s3=s3fs_central, check=False),\n",
    "                         decode_coords=\"all\",)[\"flood\"].assign_attrs(location=\"central\", resolution=20)\n",
    "flood_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run simple computation and compute the flooded area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flooded_area_ = flood_map.sum()*20*20/1000.\n",
    "flooded_area_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we haven't computed anything, so lets do the computation now on the Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flooded_area = client.compute(flooded_area_, sync=True)\n",
    "console.print(f\"Flooded area: {flooded_area.data}km2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data stored in S3 bucket at LUMI bridge (Finland).\n",
    "Data we want to read is a datacube generated from ERA-5 representing predicted rainfall data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rainfall = xr.open_zarr(store=s3fs.S3Map(root=f\"increment1-testdata/predicted_rainfall.zarr\",\n",
    "                                         s3=s3fs_lumi,\n",
    "                                         check=False),\n",
    "                        decode_coords=\"all\",)[\"tp\"].assign_attrs(location=\"lumi\", resolution=20)\n",
    "rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from attr import dataclass\n",
    "\n",
    "def accum_rain_predictions(rain_data, startdate, enddate, extent):\n",
    "    rain_ = rain_data.sel(time=slice(startdate, enddate),\n",
    "                          latitude=slice(extent.max_y, extent.min_y),\n",
    "                          longitude=slice(extent.min_x, extent.max_x))\n",
    "    return rain_.cumsum(dim=\"time\", keep_attrs=True)*1000\n",
    "\n",
    "@dataclass\n",
    "class Extent:\n",
    "    min_x: float\n",
    "    min_y: float\n",
    "    max_x: float\n",
    "    max_y: float\n",
    "    crs: str\n",
    "\n",
    "# compute accumulated rainfall over Pakistan\n",
    "roi_extent = Extent(65, 21, 71, 31, crs='EPSG:4326')\n",
    "acc_rain_ = accum_rain_predictions(rainfall, startdate=datetime(2022, 8, 18),\n",
    "                                             enddate=datetime(2022, 8, 30),\n",
    "                                             extent=roi_extent)\n",
    "\n",
    "# compute average rainfall for August 2022\n",
    "rain_ = rainfall.sel(time=slice(datetime(2022, 8, 1), datetime(2022, 8, 30))).mean(dim=\"time\", keep_attrs=True)*1000\n",
    "rain_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again run the computation on our EODC Dask cluster.\n",
    "First we compute the accumulated rainfall over Pakistan.\n",
    "Secondly we compute the average rainfall for August 2022 (monthly mean) at global scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_rain = client.compute(acc_rain_, sync=True)\n",
    "acc_rain\n",
    "mean_rain = client.compute(rain_, sync=True)\n",
    "mean_rain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of the accumlated rainfall computed for Pakistan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rain.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.close(shutdown=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
